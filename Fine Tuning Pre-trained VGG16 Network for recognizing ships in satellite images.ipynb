{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " <h1 style=\"color:brown;\">Ship detection from Satellite images using VGG16 pre-trained model</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following exercise, I will train an output layer over the pre-trained 2014 Image-Net winning VGG16 model. The objective of this exercise is to demonstrate the power of transfer learning, and how to use the VGG16 keras API call. The task at hand is binary classification on different 80x80 satellite images. We are to determine whether the image contains a ship or does not contain one.\n",
    "\n",
    "Dataset at https://www.kaggle.com/rhammell/ships-in-satellite-imagery."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is downloaded as a zip file and is then split into. A positive set and a negative set. A simple mv grep command is used since the names of the files contains their label. We now have our home directory which contains two other directories, one for images missing boats, and one for images that contains them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of positives:\n",
      "572\n",
      "number of negatives:\n",
      "1588\n",
      "number of validation positives:\n",
      "128\n",
      "number of validation negatives:\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "# we set the data directory path\n",
    "path = \"/home/ubuntu/data/\"\n",
    "# we take a look at the directories\n",
    "!echo \"number of positives:\" && ls \"/home/ubuntu/data/train/boat_present\" | wc -l \n",
    "!echo \"number of negatives:\" && ls \"/home/ubuntu/data/train/boat_missing\" | wc -l \n",
    "!echo \"number of validation positives:\" && ls \"/home/ubuntu/data/validation/boat_present_validation\" | wc -l \n",
    "!echo \"number of validation negatives:\" && ls \"/home/ubuntu/data/validation/boat_missing_validation\" | wc -l "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "So we have a total of 2800 samples, which is a somewhat small dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model, we first predict bottleneck features on a pre-trained VGG16 model so that we can then use those features to train the output layer we will add. \n",
    "\n",
    "First, let us import some useful libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n"
     ]
    }
   ],
   "source": [
    "# widely-used array operation and manipulation library\n",
    "import numpy as np \n",
    "# For preprocessing .png files\n",
    "from keras.preprocessing.image import ImageDataGenerator \n",
    "# Building blocks for our top layer\n",
    "from keras.models import Sequential, Model\n",
    "from keras import optimizers\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "# To import our VGG16 model\n",
    "from keras import applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize some useful variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dimensions of our images\n",
    "img_width, img_height = 80, 80\n",
    "\n",
    "top_model_weights_path = 'bottleneck_features.h5'\n",
    "train_data_dir = '/home/ubuntu/data/train'\n",
    "validation_data_dir = '/home/ubuntu/data/validation'\n",
    "#set number of training examples\n",
    "nb_train_samples = 2160\n",
    "nb_validation_samples = 640\n",
    "#set number of epochs\n",
    "epochs = 10\n",
    "#set batch size\n",
    "batch_size = 16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create .npy files of every input image using the ImageDataGenerator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_bottleneck_features():\n",
    "    datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "    # build the VGG16 network with custom input_shape\n",
    "    model = applications.VGG16(include_top=False, weights='imagenet', input_shape=(80,80,3))\n",
    "\n",
    "    generator = datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False) # we do not want to shuffle here since labels are going to be added afterwards\n",
    "    bottleneck_features_train = model.predict_generator(\n",
    "        generator, nb_train_samples // batch_size)\n",
    "    np.save(open('bottleneck_features.npy', 'w'),\n",
    "            bottleneck_features_train)\n",
    "    \n",
    "    generator = datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False) # we do not want to shuffle here since labels are going to be added afterwards\n",
    "    bottleneck_features_validation = model.predict_generator(\n",
    "        generator, nb_validation_samples // batch_size)\n",
    "    np.save(open('bottleneck_features_validation.npy', 'w'),\n",
    "            bottleneck_features_validation)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_top_model():\n",
    "    train_data = np.load(open('bottleneck_features.npy'))\n",
    "    train_labels = np.array(\n",
    "        [0] * (1588) + [1] * (572))\n",
    "    \n",
    "    validation_data = np.load(open('bottleneck_features_validation.npy'))\n",
    "    validation_labels = np.array(\n",
    "        [0] * (512) + [1] *(128))\n",
    "    \n",
    "    #create our top layer and train it\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(train_data, train_labels,\n",
    "              epochs=epochs,\n",
    "              batch_size=batch_size,\n",
    "              validation_data=(validation_data, validation_labels), shuffle=True)\n",
    "    model.save_weights(top_model_weights_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2160 images belonging to 2 classes.\n",
      "Found 640 images belonging to 2 classes.\n",
      "Train on 2160 samples, validate on 640 samples\n",
      "Epoch 1/10\n",
      "2160/2160 [==============================] - 1s 234us/step - loss: 0.2125 - acc: 0.9176 - val_loss: 0.1318 - val_acc: 0.9563\n",
      "Epoch 2/10\n",
      "2160/2160 [==============================] - 1s 248us/step - loss: 0.1208 - acc: 0.9639 - val_loss: 0.0526 - val_acc: 0.9797\n",
      "Epoch 3/10\n",
      "2160/2160 [==============================] - 1s 251us/step - loss: 0.0969 - acc: 0.9741 - val_loss: 0.0762 - val_acc: 0.9812\n",
      "Epoch 4/10\n",
      "2160/2160 [==============================] - 1s 251us/step - loss: 0.0907 - acc: 0.9755 - val_loss: 0.1394 - val_acc: 0.9547\n",
      "Epoch 5/10\n",
      "2160/2160 [==============================] - 1s 253us/step - loss: 0.0733 - acc: 0.9810 - val_loss: 0.1830 - val_acc: 0.9688\n",
      "Epoch 6/10\n",
      "2160/2160 [==============================] - 1s 256us/step - loss: 0.0764 - acc: 0.9769 - val_loss: 0.0763 - val_acc: 0.9812\n",
      "Epoch 7/10\n",
      "2160/2160 [==============================] - 1s 260us/step - loss: 0.0563 - acc: 0.9829 - val_loss: 0.0606 - val_acc: 0.9828\n",
      "Epoch 8/10\n",
      "2160/2160 [==============================] - 1s 255us/step - loss: 0.0678 - acc: 0.9815 - val_loss: 0.0826 - val_acc: 0.9828\n",
      "Epoch 9/10\n",
      "2160/2160 [==============================] - 1s 256us/step - loss: 0.0625 - acc: 0.9847 - val_loss: 0.1242 - val_acc: 0.9781\n",
      "Epoch 10/10\n",
      "2160/2160 [==============================] - 1s 255us/step - loss: 0.0479 - acc: 0.9894 - val_loss: 0.0887 - val_acc: 0.9781\n"
     ]
    }
   ],
   "source": [
    "# we call both functions\n",
    "save_bottleneck_features()\n",
    "train_top_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that our model achieves around 97-98% accuracy on our validation set. That is not too bad for just training a fully connected layer on top of the pre-trained VGG16 model. Now we need to connect the new top layer and the chopped off VGG16 model. As we will see, it is very straightforward. We first create the same VGG16 base model that was used when predicting the bottleneck features, then do the same for our fully-connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "base_model = applications.VGG16(weights='imagenet', include_top=False, input_shape=(80,80,3))\n",
    "print('Model loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we recreate our top layer with the already trained weights:\n",
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
    "top_model.add(Dense(512, activation='relu'))\n",
    "#top_model.add(Dropout(0.5))\n",
    "top_model.add(Dense(128, activation='relu'))\n",
    "#top_model.add(Dropout(0.5))\n",
    "top_model.add(Dense(1, activation='sigmoid'))\n",
    "top_model.load_weights(top_model_weights_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We freeze the layers up most of the VGG16 layers, so that we can fine tune our model without retraining the pre-trained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for layer in base_model.layers[:14]:\n",
    "    layer.trainable = False\n",
    "# the add method does not work anymore as of Keras 2.0.6\n",
    "model = Model(inputs= base_model.input, outputs= top_model(base_model.output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2160 images belonging to 2 classes.\n",
      "Found 640 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:34: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras.pre..., epochs=10, steps_per_epoch=135, validation_data=<keras.pre..., shuffle=True)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "135/135 [==============================] - 16s 119ms/step - loss: 0.0654 - acc: 0.9806 - val_loss: 0.1266 - val_acc: 0.9688\n",
      "Epoch 2/10\n",
      "135/135 [==============================] - 16s 117ms/step - loss: 0.0230 - acc: 0.9921 - val_loss: 0.2377 - val_acc: 0.9578\n",
      "Epoch 3/10\n",
      " 72/135 [===============>..............] - ETA: 6s - loss: 0.0409 - acc: 0.9861"
     ]
    }
   ],
   "source": [
    "\n",
    "# add in data augmentation to deal with small set\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True) \n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# fine-tune the model\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    samples_per_epoch=nb_train_samples,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems our validation_accuracy is slightly better than before. Fine tuning seems to work on the limited data that we have. Usually the fine tuning step would be run just once because of its slow gradient, but I deemed it appropriate run through ten more epochs to confirm the improvement.\n",
    "\n",
    "Now lets look at some predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "def get_batches(dirname, gen=image.ImageDataGenerator(), shuffle=True, \n",
    "                batch_size=batch_size, class_mode='categorical'):\n",
    "    return gen.flow_from_directory(path+dirname, target_size=(80,80), \n",
    "                class_mode=class_mode, shuffle=shuffle, batch_size=batch_size)\n",
    "\n",
    "batches = get_batches(\"validation/\", batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def grid_display_predictions(list_of_images, list_of_titles=[], no_of_columns=2, figsize=(7,7)):\n",
    "\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    column = 0\n",
    "    for i in range(len(list_of_images)):\n",
    "        column += 1\n",
    "        #  check for end of column and create a new figure\n",
    "        if column == no_of_columns+1:\n",
    "            fig = plt.figure(figsize=figsize)\n",
    "            column = 1\n",
    "        fig.add_subplot(1, no_of_columns, column)\n",
    "        plt.imshow(list_of_images[i])\n",
    "        plt.axis('off')\n",
    "        if len(list_of_titles) >= len(list_of_images):\n",
    "            plt.title(list_of_titles[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imgs, labels = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, img in enumerate(imgs):\n",
    "    print labels[i]\n",
    "    model.predict(img)\n",
    "    grid_display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
